{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, BatchNormalization, Conv1D, Bidirectional\n",
    "from tensorflow.keras.models import load_model\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, SpatialDropout1D, Add,\n",
    "    GRU, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D, Dense,\n",
    "    Dropout, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    ")\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTrafficPipeline:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = None\n",
    "        self.model = None\n",
    "        self.path = None\n",
    "        \n",
    "    def download_dataset(self):\n",
    "        \"\"\"Download dataset using kagglehub.\"\"\"\n",
    "        self.path = kagglehub.dataset_download(\"dhoogla/unswnb15\")\n",
    "        print(\"Dataset downloaded to:\", self.path)\n",
    "        return self.path\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load training and testing datasets.\"\"\"\n",
    "        train_data = pd.read_parquet(f\"{self.path}/UNSW_NB15_training-set.parquet\")\n",
    "        test_data = pd.read_parquet(f\"{self.path}/UNSW_NB15_testing-set.parquet\")\n",
    "        return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: C:\\Users\\Administrator\\.cache\\kagglehub\\datasets\\dhoogla\\unswnb15\\versions\\5\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = NetworkTrafficPipeline()\n",
    "    \n",
    "# Download and load data\n",
    "path = pipeline.download_dataset()\n",
    "train_data, test_data = pipeline.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "class UNSWPreprocessor:\n",
    "    def __init__(self, data: pd.DataFrame, test_data: pd.DataFrame, val_size: float = 0.2):\n",
    "        self.data = data.copy()\n",
    "        self.test_data = test_data.copy()\n",
    "        self.val_size = val_size\n",
    "        self.scalers = {\n",
    "            'robust': RobustScaler(),\n",
    "            'standard': StandardScaler()\n",
    "        }\n",
    "        self.label_encoder: Dict[str, LabelEncoder] = {}\n",
    "        self.categorical_columns = ['proto', 'service', 'state']\n",
    "        self.selected_features = None\n",
    "        self.feature_selector = None\n",
    "        self.merge_threshold = 0.05\n",
    "        self.merged_classes_mapping = None\n",
    "        self.bounds = {}\n",
    "        \n",
    "        # Correlation analysis thresholds\n",
    "        self.high_corr_threshold = 0.9\n",
    "        self.correlation_pairs = [\n",
    "            ('sbytes', 'spkts'), ('sbytes', 'sloss'), \n",
    "            ('dbytes', 'dpkts'), ('dbytes', 'dloss'),\n",
    "            ('dwin', 'swin'), ('synack', 'tcprtt'),\n",
    "            ('ct_dst_sport_ltm', 'ct_src_dport_ltm')\n",
    "        ]\n",
    "        \n",
    "        # Important features based on correlation with target\n",
    "        self.important_features = [\n",
    "            'dload', 'ct_dst_sport_ltm', 'dmean', 'rate',\n",
    "            'ct_src_dport_ltm', 'is_sm_ips_ports'\n",
    "        ]\n",
    "\n",
    "    def handle_unknown_labels(self, data: pd.DataFrame, col: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Handles unknown labels by assigning them a default value (-1)\n",
    "        \"\"\"\n",
    "        encoded_column = []\n",
    "        for value in data[col]:\n",
    "            if value in self.label_encoder[col].classes_:\n",
    "                encoded_column.append(self.label_encoder[col].transform([value])[0])\n",
    "            else:\n",
    "                encoded_column.append(-1)\n",
    "        return np.array(encoded_column)\n",
    "\n",
    "    def split_train_val(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        return train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.val_size,\n",
    "            stratify=y,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    def add_engineered_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        eps = 1e-8\n",
    "        \n",
    "        # Basic rate and ratio features\n",
    "        data['bytes_per_packet'] = (data['sbytes'] + data['dbytes']) / (data['spkts'] + data['dpkts'] + eps)\n",
    "        data['bytes_ratio'] = np.log1p(data['sbytes']) - np.log1p(data['dbytes'])\n",
    "        data['packets_ratio'] = np.log1p(data['spkts']) - np.log1p(data['dpkts'])\n",
    "        data['packet_rate'] = (data['spkts'] + data['dpkts']) / (data['dur'] + eps)\n",
    "        data['byte_rate'] = (data['sbytes'] + data['dbytes']) / (data['dur'] + eps)\n",
    "        \n",
    "        # Source and destination specific features\n",
    "        data['srate'] = data['spkts'] / (data['dur'] + eps)\n",
    "        data['drate'] = data['dpkts'] / (data['dur'] + eps)\n",
    "        data['sload'] = (data['sbytes'] * data['srate']) / (eps + data['dur'])\n",
    "        data['dload'] = (data['dbytes'] * data['drate']) / (eps + data['dur'])\n",
    "        \n",
    "        # Loss-based features\n",
    "        data['total_loss_ratio'] = (data['sloss'] + data['dloss']) / (data['spkts'] + data['dpkts'] + eps)\n",
    "        data['loss_ratio_diff'] = (data['sloss'] / (data['spkts'] + eps)) - (data['dloss'] / (data['dpkts'] + eps))\n",
    "        \n",
    "        # Connection-based features\n",
    "        if 'ct_state_ttl' in data.columns:\n",
    "            data['conn_ratio'] = data['ct_src_dport_ltm'] / (data['ct_dst_sport_ltm'] + eps)\n",
    "            data['state_ratio'] = data['ct_state_ttl'] / (data['ct_dst_ltm'] + eps)\n",
    "            data['ct_dst_src_ratio'] = data['ct_dst_ltm'] / (data['ct_src_dport_ltm'] + eps)\n",
    "        \n",
    "        # Statistical features\n",
    "        data['mean_diff'] = data['smean'] - data['dmean']\n",
    "        data['jitter_ratio'] = np.log1p(data['sjit']) - np.log1p(data['djit'])\n",
    "        \n",
    "        # Complexity features\n",
    "        if 'swin' in data.columns and 'dwin' in data.columns:\n",
    "            data['protocol_complexity'] = data['swin'] * data['dwin']\n",
    "        if 'tcprtt' in data.columns and 'synack' in data.columns:\n",
    "            data['tcp_behaviour'] = (data['tcprtt'] * data['synack']) / (data['dur'] + eps)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def handle_outliers(self, data: pd.DataFrame, columns: List[str], is_test: bool = False) -> pd.DataFrame:\n",
    "        data_clean = data.copy()\n",
    "        iqr_factor = 2.0 if is_test else 1.5\n",
    "        \n",
    "        for column in columns:\n",
    "            Q1 = self.bounds[column]['Q1']\n",
    "            Q3 = self.bounds[column]['Q3']\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - iqr_factor * IQR\n",
    "            upper_bound = Q3 + iqr_factor * IQR\n",
    "            \n",
    "            if is_test:\n",
    "                lower_bound = min(lower_bound, data_clean[column].min())\n",
    "                upper_bound = max(upper_bound, data_clean[column].max())\n",
    "            \n",
    "            data_clean[column] = data_clean[column].clip(lower_bound, upper_bound)\n",
    "            \n",
    "        return data_clean\n",
    "\n",
    "    def create_class_mapping(self, y: pd.Series) -> Dict[str, str]:\n",
    "        class_counts = y.value_counts(normalize=True)\n",
    "        classes_to_merge = class_counts[class_counts < self.merge_threshold].index\n",
    "        return {cls: 'other_attack' for cls in classes_to_merge}\n",
    "\n",
    "    def merge_minority_classes(self, y: pd.Series) -> pd.Series:\n",
    "        if self.merged_classes_mapping is None:\n",
    "            self.merged_classes_mapping = self.create_class_mapping(y)\n",
    "        return y.replace(self.merged_classes_mapping)\n",
    "\n",
    "    def balance_classes(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Balance classes using a combination of undersampling and oversampling.\n",
    "        Automatically switches between SMOTE and SMOTENC based on feature types.\n",
    "        \"\"\"\n",
    "        # First use Tomek Links to remove majority class noise\n",
    "        tl = TomekLinks(sampling_strategy='majority')\n",
    "        X_cleaned, y_cleaned = tl.fit_resample(X, y)\n",
    "        \n",
    "        # Check if we have any categorical features left after feature selection\n",
    "        categorical_features = [X.columns.get_loc(col) for col in self.categorical_columns \n",
    "                              if col in X.columns]\n",
    "        \n",
    "        if categorical_features:\n",
    "            # Use SMOTENC if we have categorical features\n",
    "            smote = SMOTENC(\n",
    "                categorical_features=categorical_features, \n",
    "                sampling_strategy='auto',\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            # Use regular SMOTE if we only have numerical features\n",
    "            smote = SMOTE(\n",
    "                sampling_strategy='auto',\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "        return smote.fit_resample(X_cleaned, y_cleaned)\n",
    "\n",
    "    def select_features(self, X: pd.DataFrame, y: pd.Series, n_features: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Select features while ensuring categorical features are preserved if needed.\n",
    "        \"\"\"\n",
    "        if self.feature_selector is None:\n",
    "            # Ensure we keep at least one categorical feature for SMOTENC\n",
    "            categorical_cols_to_keep = [col for col in self.categorical_columns if col in X.columns]\n",
    "            \n",
    "            # Dynamic feature selection based on data dimensionality\n",
    "            if n_features is None:\n",
    "                n_features = min(int(np.sqrt(len(X.columns)) * 2), len(X.columns))\n",
    "                # Ensure we have space for categorical features\n",
    "                n_features = max(n_features, len(categorical_cols_to_keep))\n",
    "            \n",
    "            # Remove highly correlated features\n",
    "            features_to_drop = []\n",
    "            corr_matrix = X.corr().abs()\n",
    "            \n",
    "            for pair in self.correlation_pairs:\n",
    "                if pair[0] in X.columns and pair[1] in X.columns:\n",
    "                    if corr_matrix.loc[pair[0], pair[1]] > self.high_corr_threshold:\n",
    "                        # Don't drop if it's a categorical feature we want to keep\n",
    "                        if pair[0] not in categorical_cols_to_keep and pair[0] not in self.important_features:\n",
    "                            features_to_drop.append(pair[0])\n",
    "                        elif pair[1] not in categorical_cols_to_keep:\n",
    "                            features_to_drop.append(pair[1])\n",
    "            \n",
    "            X = X.drop(columns=features_to_drop)\n",
    "            \n",
    "            # Sample for large datasets\n",
    "            if len(X) > 50000:\n",
    "                X_sample = X.sample(n=50000, random_state=42)\n",
    "                y_sample = y[X_sample.index]\n",
    "            else:\n",
    "                X_sample, y_sample = X, y\n",
    "            \n",
    "            # Select features while preserving categorical columns\n",
    "            non_categorical_cols = [col for col in X.columns if col not in categorical_cols_to_keep]\n",
    "            n_features_to_select = n_features - len(categorical_cols_to_keep)\n",
    "            \n",
    "            if n_features_to_select > 0 and non_categorical_cols:\n",
    "                self.feature_selector = SelectKBest(\n",
    "                    score_func=lambda X, y: mutual_info_classif(X, y, n_jobs=-1),\n",
    "                    k=n_features_to_select\n",
    "                )\n",
    "                # Fit only on non-categorical features\n",
    "                X_non_cat = X_sample[non_categorical_cols]\n",
    "                self.feature_selector.fit(X_non_cat, y_sample)\n",
    "                \n",
    "                # Get selected features\n",
    "                selected_non_cat = X_non_cat.columns[self.feature_selector.get_support()].tolist()\n",
    "                self.selected_features = categorical_cols_to_keep + selected_non_cat\n",
    "            else:\n",
    "                self.selected_features = X.columns.tolist()\n",
    "            \n",
    "        return X[self.selected_features]\n",
    "\n",
    "    def preprocess_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "        # Remove redundant or low variance columns\n",
    "        columns_to_drop = ['swin', 'stcpb', 'dtcpb', 'dwin', 'is_sm_ips_ports', \n",
    "                          'ct_flw_http_mthd', 'label']\n",
    "        \n",
    "        for df in [self.data, self.test_data]:\n",
    "            df.drop(columns=[col for col in columns_to_drop if col in df.columns], \n",
    "                   inplace=True)\n",
    "            df = self.add_engineered_features(df)\n",
    "        \n",
    "        # Handle categorical features\n",
    "        for col in self.categorical_columns:\n",
    "            if col in self.data.columns:\n",
    "                self.label_encoder[col] = LabelEncoder()\n",
    "                self.data[col] = self.label_encoder[col].fit_transform(self.data[col])\n",
    "                if col in self.test_data.columns:\n",
    "                    self.test_data[col] = self.handle_unknown_labels(self.test_data, col)\n",
    "        \n",
    "        X = self.data.drop(['attack_cat'], axis=1)\n",
    "        y = self.data['attack_cat']\n",
    "        X_test = self.test_data.drop(['attack_cat'], axis=1)\n",
    "        y_test = self.test_data['attack_cat']\n",
    "        \n",
    "        # Calculate bounds for outlier handling\n",
    "        numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "        for column in numerical_columns:\n",
    "            Q1 = X[column].quantile(0.25)\n",
    "            Q3 = X[column].quantile(0.75)\n",
    "            self.bounds[column] = {'Q1': Q1, 'Q3': Q3}\n",
    "        \n",
    "        # Handle outliers\n",
    "        X = self.handle_outliers(X, numerical_columns)\n",
    "        X_test = self.handle_outliers(X_test, numerical_columns, is_test=True)\n",
    "        \n",
    "        # Process target variable\n",
    "        y = self.merge_minority_classes(y)\n",
    "        y_test = self.merge_minority_classes(y_test)\n",
    "        self.label_encoder['attack_cat'] = LabelEncoder()\n",
    "        y = pd.Series(self.label_encoder['attack_cat'].fit_transform(y), index=y.index)\n",
    "        y_test = self.handle_unknown_labels(pd.DataFrame({'attack_cat': y_test}), 'attack_cat')\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = self.split_train_val(X, y)\n",
    "        \n",
    "        # Select features\n",
    "        X_train = self.select_features(X_train, y_train)\n",
    "        X_val = X_val[self.selected_features]\n",
    "        X_test = X_test[self.selected_features]\n",
    "        \n",
    "        # Scale features\n",
    "        for scaler_name, scaler in self.scalers.items():\n",
    "            if scaler_name == 'robust':\n",
    "                cols_to_scale = [col for col in X_train.columns if col not in self.categorical_columns]\n",
    "            else:  # standard scaler for important features\n",
    "                cols_to_scale = [col for col in X_train.columns if col in self.important_features]\n",
    "            \n",
    "            if cols_to_scale:\n",
    "                X_train[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
    "                X_val[cols_to_scale] = scaler.transform(X_val[cols_to_scale])\n",
    "                X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "        \n",
    "        # Balance classes\n",
    "        X_train_balanced, y_train_balanced = self.balance_classes(X_train, y_train)\n",
    "        \n",
    "        return X_train_balanced, X_val, X_test, y_train_balanced, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = UNSWPreprocessor(train_data, test_data, val_size=0.2)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "class AutoMLEnsembleHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes, sequence_length):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def build(self, hp):\n",
    "        inputs = tf.keras.Input(shape=(self.sequence_length, self.input_shape))\n",
    "        \n",
    "        # Hyperparameters for architecture\n",
    "        base_filters = hp.Int('base_filters', 32, 128, step=32)\n",
    "        num_residual_blocks = hp.Int('num_residual_blocks', 1, 3)\n",
    "        num_attention_heads = hp.Int('num_attention_heads', 4, 16, step=4)\n",
    "        dense_units = hp.Int('dense_units', 64, 512, step=64)\n",
    "        dropout_rate = hp.Float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "        \n",
    "        # 1. Residual Blocks\n",
    "        x = inputs\n",
    "        for _ in range(num_residual_blocks):\n",
    "            x = self._create_residual_block(x, base_filters, hp)\n",
    "            \n",
    "        # 2. Temporal Block\n",
    "        x = self._create_temporal_block(x, base_filters, hp)\n",
    "        \n",
    "        # 3. Multi-Scale Block\n",
    "        x = self._create_multi_scale_block(x, base_filters, hp)\n",
    "        \n",
    "        # 4. Attention Block\n",
    "        x = self._create_attention_block(x, base_filters, num_attention_heads)\n",
    "        \n",
    "        # 5. Global Pooling\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # 6. Dense Layers\n",
    "        num_dense_layers = hp.Int('num_dense_layers', 1, 3)\n",
    "        for i in range(num_dense_layers):\n",
    "            x = layers.Dense(dense_units // (2**i))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Activation(hp.Choice(f'dense_activation_{i}', ['relu', 'swish']))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = Model(inputs, outputs)\n",
    "        \n",
    "        # Compile model\n",
    "        learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _create_residual_block(self, x, filters, hp):\n",
    "        kernel_size = hp.Int('res_kernel_size', 3, 7, step=2)\n",
    "        activation = hp.Choice('res_activation', ['relu', 'swish'])\n",
    "        \n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(activation)(x)\n",
    "        x = layers.SpatialDropout1D(0.2)(x)\n",
    "        \n",
    "        if x.shape[-1] != shortcut.shape[-1]:\n",
    "            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "            \n",
    "        return layers.Add()([x, shortcut])\n",
    "    \n",
    "    def _create_temporal_block(self, x, filters, hp):\n",
    "        gru_units = hp.Int('gru_units', filters//2, filters, step=32)\n",
    "        bidirectional = hp.Boolean('bidirectional')\n",
    "        \n",
    "        if bidirectional:\n",
    "            x = layers.Bidirectional(layers.GRU(gru_units, return_sequences=True))(x)\n",
    "        else:\n",
    "            x = layers.GRU(gru_units, return_sequences=True)(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def _create_multi_scale_block(self, x, filters, hp):\n",
    "        num_scales = hp.Int('num_scales', 2, 4)\n",
    "        scales = list(range(3, 3 + num_scales * 2, 2))\n",
    "        convs = []\n",
    "        \n",
    "        for scale in scales:\n",
    "            conv = layers.Conv1D(filters//len(scales), scale, padding='same')(x)\n",
    "            conv = layers.BatchNormalization()(conv)\n",
    "            conv = layers.Activation('swish')(conv)\n",
    "            convs.append(conv)\n",
    "            \n",
    "        return layers.Concatenate()(convs)\n",
    "    \n",
    "    def _create_attention_block(self, x, filters, num_heads):\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=filters//num_heads\n",
    "        )(x, x)\n",
    "        \n",
    "        x = layers.Add()([attention_output, x])\n",
    "        return layers.LayerNormalization()(x)\n",
    "\n",
    "class EnhancedAutoMLEnsemble:\n",
    "    def __init__(self, input_shape, num_classes, sequence_length=10):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "        self.deep_model = None\n",
    "        self.xgb_model = None\n",
    "        self.model_weights = None\n",
    "        \n",
    "    def optimize(self, X_train, y_train, X_val, y_val, max_trials=10):\n",
    "        \"\"\"\n",
    "        Otimiza o modelo com índices corrigidos\n",
    "        \"\"\"\n",
    "        # Resetar índices e converter para numpy se necessário\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.reset_index(drop=True)\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "        if isinstance(X_val, pd.DataFrame):\n",
    "            X_val = X_val.reset_index(drop=True)\n",
    "        if isinstance(y_val, pd.Series):\n",
    "            y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "        # 1. Optimize Deep Learning Model\n",
    "        hypermodel = AutoMLEnsembleHyperModel(\n",
    "            self.input_shape,\n",
    "            self.num_classes,\n",
    "            self.sequence_length\n",
    "        )\n",
    "        \n",
    "        tuner = RandomSearch(\n",
    "            hypermodel,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=max_trials,\n",
    "            directory='keras_tuner_dir',\n",
    "            project_name='automl_ensemble'\n",
    "        )\n",
    "        \n",
    "        stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        \n",
    "        X_train_seq = self.prepare_sequence_data(X_train)\n",
    "        X_val_seq = self.prepare_sequence_data(X_val)\n",
    "        \n",
    "        # Converter targets para numpy arrays\n",
    "        y_train_np = np.array(y_train)\n",
    "        y_val_np = np.array(y_val)\n",
    "        \n",
    "        tuner.search(\n",
    "            X_train_seq, y_train_np,\n",
    "            validation_data=(X_val_seq, y_val_np),\n",
    "            epochs=5,\n",
    "            callbacks=[stop_early]\n",
    "        )\n",
    "        \n",
    "        # Get best deep learning model\n",
    "        self.deep_model = tuner.get_best_models(1)[0]\n",
    "        \n",
    "        # 2. Extract features for XGBoost\n",
    "        train_features = self.extract_deep_features(X_train)\n",
    "        val_features = self.extract_deep_features(X_val)\n",
    "        \n",
    "        # 3. Optimize XGBoost with Optuna\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Construir o modelo\n",
    "            model = hypermodel.build(trial)\n",
    "    \n",
    "            # Treinar o modelo\n",
    "            model.fit(\n",
    "                X_train_seq, y_train_np,\n",
    "                validation_data=(X_val_seq, y_val_np),\n",
    "                epochs=5,\n",
    "                callbacks=[stop_early],\n",
    "                verbose=0\n",
    "            )\n",
    "    \n",
    "            # Fazer previsões no conjunto de validação\n",
    "            y_pred = model.predict(X_val_seq)\n",
    "            y_pred_classes = y_pred.argmax(axis=1)\n",
    "    \n",
    "            # Calcular métricas\n",
    "            accuracy = (y_pred_classes == y_val_np).mean()\n",
    "            f1 = f1_score(y_val_np, y_pred_classes, average='weighted')\n",
    "    \n",
    "            # Combinar métricas (ajuste os pesos conforme necessário)\n",
    "            weighted_score = 0.5 * accuracy + 0.5 * f1\n",
    "            return weighted_score\n",
    "        \n",
    "        study.optimize(objective, n_trials=max_trials)\n",
    "        \n",
    "        # Create final XGBoost model\n",
    "        self.xgb_model = xgb.XGBClassifier(\n",
    "            **study.best_params,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss',\n",
    "            tree_method='hist'\n",
    "        )\n",
    "        self.xgb_model.fit(train_features, y_train_np)\n",
    "        \n",
    "        # 4. Optimize ensemble weights\n",
    "        self._optimize_ensemble_weights(val_features, y_val_np)\n",
    "        \n",
    "    def prepare_sequence_data(self, X):\n",
    "        \"\"\"\n",
    "        Prepara os dados em formato de sequência, garantindo índices corretos\n",
    "        \"\"\"\n",
    "        # Converter para numpy array se for DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        elif isinstance(X, pd.Series):\n",
    "            X = X.to_numpy()\n",
    "        \n",
    "        # Garantir que X seja 2D se for 1D\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1, self.input_shape)\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "    \n",
    "        # Criar sequências\n",
    "        sequences = np.zeros((n_samples, self.sequence_length, self.input_shape))\n",
    "    \n",
    "        for i in range(n_samples):\n",
    "            start_idx = max(0, i - self.sequence_length + 1)\n",
    "            end_idx = i + 1\n",
    "            actual_seq = X[start_idx:end_idx]\n",
    "        \n",
    "            if len(actual_seq) < self.sequence_length:\n",
    "                # Preencher com o primeiro valor\n",
    "                sequences[i, :self.sequence_length-len(actual_seq)] = actual_seq[0]\n",
    "                sequences[i, self.sequence_length-len(actual_seq):] = actual_seq\n",
    "            else:\n",
    "                sequences[i] = actual_seq[-self.sequence_length:]\n",
    "    \n",
    "        return sequences\n",
    "\n",
    "    def extract_deep_features(self, X):\n",
    "        feature_model = Model(\n",
    "            inputs=self.deep_model.input,\n",
    "            outputs=self.deep_model.layers[-2].output\n",
    "        )\n",
    "        X_seq = self.prepare_sequence_data(X)\n",
    "        return feature_model.predict(X_seq)\n",
    "    \n",
    "    def _optimize_ensemble_weights(self, X_val, y_val):\n",
    "        deep_pred = self.deep_model.predict(self.prepare_sequence_data(X_val))\n",
    "        xgb_pred = self.xgb_model.predict_proba(X_val)\n",
    "        \n",
    "        def objective(trial):\n",
    "            w1 = trial.suggest_float('deep_weight', 0.2, 0.8)\n",
    "            w2 = 1 - w1\n",
    "            \n",
    "            ensemble_pred = w1 * deep_pred + w2 * xgb_pred\n",
    "            y_pred = np.argmax(ensemble_pred, axis=1)\n",
    "            return f1_score(y_val, y_pred, average='weighted')\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=100)\n",
    "        \n",
    "        self.model_weights = [study.best_params['deep_weight'],\n",
    "                            1 - study.best_params['deep_weight']]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_seq = self.prepare_sequence_data(X)\n",
    "        deep_pred = self.deep_model.predict(X_seq)\n",
    "        X_features = self.extract_deep_features(X)\n",
    "        xgb_pred = self.xgb_model.predict_proba(X_features)\n",
    "        \n",
    "        ensemble_pred = (self.model_weights[0] * deep_pred + \n",
    "                        self.model_weights[1] * xgb_pred)\n",
    "        return np.argmax(ensemble_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner import HyperModel\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "class AutoMLEnsembleHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes, sequence_length):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def build(self, trial):\n",
    "        inputs = tf.keras.Input(shape=(self.sequence_length, self.input_shape))\n",
    "    \n",
    "        # Hyperparameters for architecture\n",
    "        base_filters = trial.suggest_int('base_filters', 32, 128, step=32)\n",
    "        num_residual_blocks = trial.suggest_int('num_residual_blocks', 1, 3)\n",
    "        num_attention_heads = trial.suggest_int('num_attention_heads', 4, 16, step=4)\n",
    "        dense_units = trial.suggest_int('dense_units', 64, 512, step=64)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    \n",
    "        # 1. Residual Blocks\n",
    "        x = inputs\n",
    "        for _ in range(num_residual_blocks):\n",
    "            x = self._create_residual_block(x, base_filters, trial)\n",
    "        \n",
    "        # 2. Temporal Block\n",
    "        x = self._create_temporal_block(x, base_filters, trial)\n",
    "    \n",
    "        # 3. Multi-Scale Block\n",
    "        x = self._create_multi_scale_block(x, base_filters, trial)\n",
    "    \n",
    "        # 4. Attention Block\n",
    "        x = self._create_attention_block(x, base_filters, num_attention_heads)\n",
    "    \n",
    "        # 5. Global Pooling\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "        # 6. Dense Layers\n",
    "        num_dense_layers = trial.suggest_int('num_dense_layers', 1, 3)\n",
    "        for i in range(num_dense_layers):\n",
    "            x = layers.Dense(dense_units // (2**i))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Activation(trial.suggest_categorical(f'dense_activation_{i}', ['relu', 'swish']))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "    \n",
    "        model = Model(inputs, outputs)\n",
    "    \n",
    "        # Compile model\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "        return model\n",
    "\n",
    "    \n",
    "    def _create_residual_block(self, x, filters, hp):\n",
    "        kernel_size = hp.Int('res_kernel_size', 3, 7, step=2)\n",
    "        activation = hp.Choice('res_activation', ['relu', 'swish'])\n",
    "        \n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(activation)(x)\n",
    "        x = layers.SpatialDropout1D(0.2)(x)\n",
    "        \n",
    "        if x.shape[-1] != shortcut.shape[-1]:\n",
    "            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "            \n",
    "        return layers.Add()([x, shortcut])\n",
    "    \n",
    "    def _create_temporal_block(self, x, filters, hp):\n",
    "        gru_units = hp.Int('gru_units', filters//2, filters, step=32)\n",
    "        bidirectional = hp.Boolean('bidirectional')\n",
    "        \n",
    "        if bidirectional:\n",
    "            x = layers.Bidirectional(layers.GRU(gru_units, return_sequences=True))(x)\n",
    "        else:\n",
    "            x = layers.GRU(gru_units, return_sequences=True)(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def _create_multi_scale_block(self, x, filters, hp):\n",
    "        num_scales = hp.Int('num_scales', 2, 4)\n",
    "        scales = list(range(3, 3 + num_scales * 2, 2))\n",
    "        convs = []\n",
    "        \n",
    "        for scale in scales:\n",
    "            conv = layers.Conv1D(filters//len(scales), scale, padding='same')(x)\n",
    "            conv = layers.BatchNormalization()(conv)\n",
    "            conv = layers.Activation('swish')(conv)\n",
    "            convs.append(conv)\n",
    "            \n",
    "        return layers.Concatenate()(convs)\n",
    "    \n",
    "    def _create_attention_block(self, x, filters, num_heads):\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=filters//num_heads\n",
    "        )(x, x)\n",
    "        \n",
    "        x = layers.Add()([attention_output, x])\n",
    "        return layers.LayerNormalization()(x)\n",
    "\n",
    "class EnhancedAutoMLEnsemble:\n",
    "    def __init__(self, input_shape, num_classes, sequence_length=10):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "        self.deep_model = None\n",
    "        self.xgb_model = None\n",
    "        self.model_weights = None\n",
    "        \n",
    "    def optimize(self, X_train, y_train, X_val, y_val, max_trials=10):\n",
    "        X_train = X_train.reset_index(drop=True) if isinstance(X_train, pd.DataFrame) else X_train\n",
    "        y_train = y_train.reset_index(drop=True) if isinstance(y_train, pd.Series) else y_train\n",
    "        X_val = X_val.reset_index(drop=True) if isinstance(X_val, pd.DataFrame) else X_val\n",
    "        y_val = y_val.reset_index(drop=True) if isinstance(y_val, pd.Series) else y_val\n",
    "        \n",
    "        y_train_np = np.array(y_train)\n",
    "        y_val_np = np.array(y_val)\n",
    "        \n",
    "        hypermodel = AutoMLEnsembleHyperModel(\n",
    "            self.input_shape,\n",
    "            self.num_classes,\n",
    "            self.sequence_length\n",
    "        )\n",
    "        \n",
    "        X_train_seq = self.prepare_sequence_data(X_train)\n",
    "        X_val_seq = self.prepare_sequence_data(X_val)\n",
    "\n",
    "        def objective(trial):\n",
    "            dl_params = {\n",
    "                'base_filters': trial.suggest_int('base_filters', 32, 128, step=32),\n",
    "                'num_residual_blocks': trial.suggest_int('num_residual_blocks', 1, 3),\n",
    "                'num_attention_heads': trial.suggest_int('num_attention_heads', 4, 16, step=4),\n",
    "                'dense_units': trial.suggest_int('dense_units', 64, 512, step=64),\n",
    "                'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1),\n",
    "                'learning_rate': trial.suggest_float('dl_learning_rate', 1e-4, 1e-2, log=True)\n",
    "            }\n",
    "            \n",
    "            xgb_params = {\n",
    "                'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_float('xgb_learning_rate', 1e-3, 0.1, log=True),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "            }\n",
    "            \n",
    "            deep_model = hypermodel.build(trial)\n",
    "            deep_model.fit(\n",
    "                X_train_seq, y_train_np,\n",
    "                validation_data=(X_val_seq, y_val_np),\n",
    "                epochs=5,\n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=3)],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            deep_pred_prob = deep_model.predict(X_val_seq, verbose=0)\n",
    "            deep_pred = np.argmax(deep_pred_prob, axis=1)\n",
    "            deep_f1 = f1_score(y_val_np, deep_pred, average='weighted')\n",
    "            \n",
    "            feature_model = Model(inputs=deep_model.input, outputs=deep_model.layers[-2].output)\n",
    "            train_features = feature_model.predict(X_train_seq, verbose=0)\n",
    "            val_features = feature_model.predict(X_val_seq, verbose=0)\n",
    "            \n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                **xgb_params,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='mlogloss',\n",
    "                tree_method='hist'\n",
    "            )\n",
    "            xgb_model.fit(train_features, y_train_np)\n",
    "            xgb_pred = xgb_model.predict(val_features)\n",
    "            xgb_f1 = f1_score(y_val_np, xgb_pred, average='weighted')\n",
    "            \n",
    "            total_f1 = deep_f1 + xgb_f1\n",
    "            deep_weight = deep_f1 / total_f1 if total_f1 > 0 else 0.5\n",
    "            \n",
    "            ensemble_pred = (deep_weight * deep_pred_prob + \n",
    "                           (1 - deep_weight) * xgb_model.predict_proba(val_features))\n",
    "            final_pred = np.argmax(ensemble_pred, axis=1)\n",
    "            final_f1 = f1_score(y_val_np, final_pred, average='weighted')\n",
    "            \n",
    "            if trial.number == 0 or final_f1 > trial.study.best_value:\n",
    "                self.deep_model = deep_model\n",
    "                self.xgb_model = xgb_model\n",
    "                self.model_weights = [deep_weight, 1 - deep_weight]\n",
    "                \n",
    "            return final_f1\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            pruner=optuna.pruners.MedianPruner()\n",
    "        )\n",
    "        study.optimize(objective, n_trials=max_trials)\n",
    "    \n",
    "    def prepare_sequence_data(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        elif isinstance(X, pd.Series):\n",
    "            X = X.to_numpy()\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1, self.input_shape)\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        sequences = np.zeros((n_samples, self.sequence_length, self.input_shape))\n",
    "    \n",
    "        for i in range(n_samples):\n",
    "            start_idx = max(0, i - self.sequence_length + 1)\n",
    "            end_idx = i + 1\n",
    "            actual_seq = X[start_idx:end_idx]\n",
    "        \n",
    "            if len(actual_seq) < self.sequence_length:\n",
    "                sequences[i, :self.sequence_length-len(actual_seq)] = actual_seq[0]\n",
    "                sequences[i, self.sequence_length-len(actual_seq):] = actual_seq\n",
    "            else:\n",
    "                sequences[i] = actual_seq[-self.sequence_length:]\n",
    "    \n",
    "        return sequences\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_seq = self.prepare_sequence_data(X)\n",
    "        deep_pred = self.deep_model.predict(X_seq, verbose=0)\n",
    "        \n",
    "        feature_model = Model(\n",
    "            inputs=self.deep_model.input,\n",
    "            outputs=self.deep_model.layers[-2].output\n",
    "        )\n",
    "        X_features = feature_model.predict(X_seq, verbose=0)\n",
    "        xgb_pred = self.xgb_model.predict_proba(X_features)\n",
    "        \n",
    "        ensemble_pred = (self.model_weights[0] * deep_pred + \n",
    "                        self.model_weights[1] * xgb_pred)\n",
    "        return np.argmax(ensemble_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 09m 45s]\n",
      "val_accuracy: 0.6541674733161926\n",
      "\n",
      "Best val_accuracy So Far: 0.7033562660217285\n",
      "Total elapsed time: 02h 01m 13s\n",
      "\n",
      "Search: Running Trial #9\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "96                |64                |base_filters\n",
      "2                 |2                 |num_residual_blocks\n",
      "4                 |12                |num_attention_heads\n",
      "192               |384               |dense_units\n",
      "0.5               |0.1               |dropout_rate\n",
      "7                 |3                 |res_kernel_size\n",
      "swish             |swish             |res_activation\n",
      "16                |16                |gru_units\n",
      "True              |True              |bidirectional\n",
      "3                 |3                 |num_scales\n",
      "2                 |1                 |num_dense_layers\n",
      "relu              |relu              |dense_activation_0\n",
      "0.0071211         |0.00073687        |learning_rate\n",
      "swish             |None              |dense_activation_1\n",
      "relu              |None              |dense_activation_2\n",
      "\n",
      "Epoch 1/5\n",
      "\u001b[1m6077/9377\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 13ms/step - accuracy: 0.2025 - loss: 1.9031"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "model = EnhancedAutoMLEnsemble(\n",
    "    input_shape=X_train.shape[1],  # Número de features\n",
    "    num_classes=len(np.unique(y_train)),  # Número de classes\n",
    "    sequence_length=10  # Comprimento da sequência temporal\n",
    ")\n",
    "\n",
    "# 4. Otimize o modelo\n",
    "model.optimize(X_train, y_train, X_val, y_val, max_trials=10)\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
