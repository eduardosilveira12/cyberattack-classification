{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 04m 29s]\n",
      "val_accuracy: 0.7576491832733154\n",
      "\n",
      "Best val_accuracy So Far: 0.7576491832733154\n",
      "Total elapsed time: 00h 04m 29s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |1                 |conv_blocks\n",
      "64                |64                |filters_0\n",
      "5                 |5                 |kernel_size_0\n",
      "1                 |3                 |gru_blocks\n",
      "128               |256               |gru_units_0\n",
      "0.1               |0.4               |dropout_gru_0\n",
      "128               |64                |dense_units\n",
      "0.3               |0.4               |dropout_dense\n",
      "0.0001            |0.0001            |learning_rate\n",
      "64                |64                |gru_units_1\n",
      "0.1               |0.1               |dropout_gru_1\n",
      "256               |64                |gru_units_2\n",
      "0.2               |0.1               |dropout_gru_2\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "3                 |3                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "\u001b[1m3355/4384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.6120 - loss: 1.1268"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization, Conv1D, MaxPooling1D, GRU, Dense, \n",
    "    Dropout, Bidirectional, Flatten\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras_tuner as kt\n",
    "import kagglehub\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "\n",
    "class NetworkTrafficPipeline:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = None\n",
    "        self.model = None\n",
    "        self.path = None\n",
    "        \n",
    "    def download_dataset(self):\n",
    "        \"\"\"Download dataset using kagglehub.\"\"\"\n",
    "        self.path = kagglehub.dataset_download(\"dhoogla/unswnb15\")\n",
    "        print(\"Dataset downloaded to:\", self.path)\n",
    "        return self.path\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load training and testing datasets.\"\"\"\n",
    "        train_data = pd.read_parquet(f\"{self.path}/UNSW_NB15_training-set.parquet\")\n",
    "        test_data = pd.read_parquet(f\"{self.path}/UNSW_NB15_testing-set.parquet\")\n",
    "        return train_data, test_data\n",
    "# Preprocessor Class\n",
    "class DataAnalyzer:\n",
    "    \"\"\"\n",
    "    A comprehensive class for exploratory data analysis and feature engineering insights.\n",
    "    \n",
    "    This class provides methods for:\n",
    "    - Basic statistical analysis\n",
    "    - Correlation detection\n",
    "    - Outlier identification\n",
    "    - Feature engineering suggestions and visualization\n",
    "    - Data distribution analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with a pandas DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataset for analysis\n",
    "        \"\"\"\n",
    "        self.df = df.copy()  # Create a copy to avoid modifying original data\n",
    "        self.num_cols = self.df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        self.cat_cols = self.df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "    def basic_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform basic dataset analysis.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Contains missing values, numerical statistics, and categorical counts\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'missing_values': {\n",
    "                'count': self.df.isnull().sum(),\n",
    "                'percentage': (self.df.isnull().sum() / len(self.df) * 100).round(2)\n",
    "            },\n",
    "            'numerical_stats': self.df[self.num_cols].describe(),\n",
    "            'categorical_stats': {\n",
    "                col: {\n",
    "                    'unique_values': self.df[col].nunique(),\n",
    "                    'value_counts': self.df[col].value_counts(),\n",
    "                    'missing_percentage': (self.df[col].isnull().sum() / len(self.df) * 100).round(2)\n",
    "                } for col in self.cat_cols\n",
    "            }\n",
    "        }\n",
    "        return analysis\n",
    "\n",
    "    def identify_correlations(self, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Identify strong correlations between numerical features.\n",
    "        \n",
    "        Args:\n",
    "            threshold (float): Correlation coefficient threshold (default: 0.7)\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Pairs of highly correlated features\n",
    "        \"\"\"\n",
    "        if len(self.num_cols) < 2:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        corr_matrix = self.df[self.num_cols].corr()\n",
    "        high_corr = np.where(np.abs(corr_matrix) > threshold)\n",
    "        high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y], corr_matrix.iloc[x, y])\n",
    "                          for x, y in zip(*high_corr) if x != y and x < y]\n",
    "        return pd.DataFrame(high_corr_pairs, columns=['Feature1', 'Feature2', 'Correlation'])\n",
    "\n",
    "    def detect_outliers(self, cols=None, method='zscore', threshold=3):\n",
    "        \"\"\"\n",
    "        Detect outliers using multiple methods.\n",
    "        \n",
    "        Args:\n",
    "            cols (list): Specific columns to check (default: all numerical columns)\n",
    "            method (str): 'zscore' or 'iqr' (default: 'zscore')\n",
    "            threshold (float): Threshold for outlier detection (default: 3)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Outlier statistics for each analyzed column\n",
    "        \"\"\"\n",
    "        if cols is None:\n",
    "            cols = self.num_cols\n",
    "            \n",
    "        outliers = {}\n",
    "        for col in cols:\n",
    "            if method == 'zscore':\n",
    "                z_scores = np.abs(stats.zscore(self.df[col].dropna()))\n",
    "                outlier_mask = z_scores > threshold\n",
    "            else:  # IQR method\n",
    "                Q1 = self.df[col].quantile(0.25)\n",
    "                Q3 = self.df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                outlier_mask = ((self.df[col] < (Q1 - 1.5 * IQR)) | \n",
    "                              (self.df[col] > (Q3 + 1.5 * IQR)))\n",
    "                \n",
    "            outliers[col] = {\n",
    "                'count': outlier_mask.sum(),\n",
    "                'percentage': (outlier_mask.sum() / len(self.df[col].dropna()) * 100).round(2),\n",
    "                'indexes': self.df[outlier_mask].index.tolist(),\n",
    "                'range': {\n",
    "                    'min': self.df[col][~outlier_mask].min(),\n",
    "                    'max': self.df[col][~outlier_mask].max()\n",
    "                }\n",
    "            }\n",
    "        return outliers\n",
    "\n",
    "    def analyze_class_distribution(self, data, plot=True):\n",
    "        \"\"\"\n",
    "        Analyze the distribution of classes in a dataset.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Dataset \n",
    "            plot (bool): If True, display a bar plot of class distribution (default: True)\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series: Contagem de cada classe\n",
    "        \"\"\"\n",
    "        class_distribution = data['attack_cat'].value_counts()\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.title('Attack Category Distribution')\n",
    "            plt.xlabel('Attack Category')\n",
    "            plt.ylabel('Count')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Gráfico de pizza para proporção normal vs. ataque\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            attack_prop = data['label'].value_counts()\n",
    "            plt.pie(attack_prop, labels=['Normal', 'Attack'], autopct='%1.1f%%')\n",
    "            plt.title('Normal vs. Attack Traffic Distribution')\n",
    "            plt.show()\n",
    "        \n",
    "        return class_distribution\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self.train_data = train_data.copy()\n",
    "        self.test_data = test_data.copy()\n",
    "        self.scalers = {\n",
    "            'standard': StandardScaler(),\n",
    "            'robust': RobustScaler()\n",
    "        }\n",
    "        self.label_encoder = {}\n",
    "        self.categorical_columns = ['proto', 'service', 'state']\n",
    "        self.selected_features = None\n",
    "        self.feature_selector = None\n",
    "\n",
    "    def add_engineered_features(self, data):\n",
    "        \"\"\"Add engineered features to improve model performance.\"\"\"\n",
    "        # Packet-based features\n",
    "        data['bytes_per_packet'] = data['sbytes'] / (data['spkts'] + 1e-8)\n",
    "        data['packet_size_ratio'] = (data['sbytes'] + 1e-8) / (data['dbytes'] + 1e-8)\n",
    "        \n",
    "        # Time-based features\n",
    "        data['bytes_per_second'] = (data['sbytes'] + data['dbytes']) / (data['dur'] + 1e-8)\n",
    "        data['packets_per_second'] = (data['spkts'] + data['dpkts']) / (data['dur'] + 1e-8)\n",
    "        \n",
    "        # Rate-based features\n",
    "        data['srate'] = data['spkts'] / (data['dur'] + 1e-8)\n",
    "        data['drate'] = data['dpkts'] / (data['dur'] + 1e-8)\n",
    "        \n",
    "        # Statistical features\n",
    "        data['byte_ratio'] = np.log1p(data['sbytes']) - np.log1p(data['dbytes'])\n",
    "        data['packet_ratio'] = np.log1p(data['spkts']) - np.log1p(data['dpkts'])\n",
    "        \n",
    "        # Interaction features\n",
    "        data['sload'] = (data['sbytes'] * data['srate']) / (1e-8 + data['dur'])\n",
    "        data['dload'] = (data['dbytes'] * data['drate']) / (1e-8 + data['dur'])\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def handle_outliers(self, data, columns, method='iqr'):\n",
    "        \"\"\"Handle outliers using IQR method.\"\"\"\n",
    "        for column in columns:\n",
    "            Q1 = data[column].quantile(0.25)\n",
    "            Q3 = data[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            data[column] = data[column].clip(lower_bound, upper_bound)\n",
    "        return data\n",
    "\n",
    "    def select_features(self, X, y, n_features=24):\n",
    "        \"\"\"Select most important features using mutual information.\"\"\"\n",
    "        if self.feature_selector is None:\n",
    "            self.feature_selector = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "            self.feature_selector.fit(X, y)\n",
    "            self.selected_features = X.columns[self.feature_selector.get_support()].tolist()\n",
    "        return X[self.selected_features]\n",
    "\n",
    "    def balance_classes(self, X, y):\n",
    "        \"\"\"Advanced class balancing using combination of techniques.\"\"\"\n",
    "        # Remove noise using Tomek links\n",
    "        tl = TomekLinks(sampling_strategy='majority')\n",
    "        X_cleaned, y_cleaned = tl.fit_resample(X, y)\n",
    "        \n",
    "        # Apply ADASYN for minority classes\n",
    "        minority_classes = [cls for cls, count in pd.Series(y_cleaned).value_counts().items() \n",
    "                          if count < len(y_cleaned) * 0.1]\n",
    "        \n",
    "        if minority_classes:\n",
    "            sampling_strategy = {cls: int(len(y_cleaned) * 0.1) for cls in minority_classes}\n",
    "            adasyn = ADASYN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "            X_balanced, y_balanced = adasyn.fit_resample(X_cleaned, y_cleaned)\n",
    "        else:\n",
    "            X_balanced, y_balanced = X_cleaned, y_cleaned\n",
    "        \n",
    "        return X_balanced, y_balanced\n",
    "\n",
    "    def train_set_preprocess(self):\n",
    "        \"\"\"Preprocess training data.\"\"\"\n",
    "        # Remove unnecessary columns\n",
    "        columns_to_drop = ['swin', 'stcpb', 'dtcpb', 'dwin', 'is_sm_ips_ports', 'ct_flw_http_mthd']\n",
    "        self.train_data.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "        \n",
    "        # Add engineered features\n",
    "        self.train_data = self.add_engineered_features(self.train_data)\n",
    "        \n",
    "        # Handle categorical features\n",
    "        for col in self.categorical_columns:\n",
    "            self.label_encoder[col] = LabelEncoder()\n",
    "            self.train_data[col] = self.label_encoder[col].fit_transform(self.train_data[col])\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = self.train_data.drop(['attack_cat', 'label'], axis=1)\n",
    "        y = self.train_data['attack_cat']\n",
    "        \n",
    "        # Handle numerical features\n",
    "        numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "        X = self.handle_outliers(X, numerical_columns)\n",
    "        \n",
    "        # Encode target\n",
    "        self.label_encoder['attack_cat'] = LabelEncoder()\n",
    "        y = self.label_encoder['attack_cat'].fit_transform(y)\n",
    "        \n",
    "        # Feature selection\n",
    "        X = self.select_features(X, y)\n",
    "        \n",
    "        # Scale features\n",
    "        X = pd.DataFrame(self.scalers['robust'].fit_transform(X), columns=X.columns)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def test_set_preprocess(self):\n",
    "        \"\"\"Preprocess test data.\"\"\"\n",
    "        self.test_data.drop(columns=['swin', 'stcpb', 'dtcpb', 'dwin', 'is_sm_ips_ports', 'ct_flw_http_mthd'], \n",
    "                           errors='ignore', inplace=True)\n",
    "        \n",
    "        self.test_data = self.add_engineered_features(self.test_data)\n",
    "        \n",
    "        for col in self.categorical_columns:\n",
    "            known_classes = set(self.label_encoder[col].classes_)\n",
    "            self.test_data[col] = self.test_data[col].map(lambda x: x if x in known_classes else list(known_classes)[0])\n",
    "            self.test_data[col] = self.label_encoder[col].transform(self.test_data[col])\n",
    "        \n",
    "        X = self.test_data.drop(['attack_cat', 'label'], axis=1)\n",
    "        y = self.test_data['attack_cat']\n",
    "        \n",
    "        numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "        X = self.handle_outliers(X, numerical_columns)\n",
    "        \n",
    "        known_classes = set(self.label_encoder['attack_cat'].classes_)\n",
    "        y = y.map(lambda x: x if x in known_classes else list(known_classes)[0])\n",
    "        y = self.label_encoder['attack_cat'].transform(y)\n",
    "        \n",
    "        X = X[self.selected_features]\n",
    "        X = pd.DataFrame(self.scalers['robust'].transform(X), columns=X.columns)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "# CNN-GRU Model with AutoML\n",
    "class CNNGRUAutoML:\n",
    "    def __init__(self, input_shape, num_classes, sequence_length=10):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def build_model(self, hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        for i in range(hp.Int(\"conv_blocks\", 1, 3)):\n",
    "            model.add(Conv1D(\n",
    "                filters=hp.Choice(f\"filters_{i}\", [32, 64, 128]),\n",
    "                kernel_size=hp.Choice(f\"kernel_size_{i}\", [3, 5]),\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\"))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "        \n",
    "        # GRU layers\n",
    "        for i in range(hp.Int(\"gru_blocks\", 1, 3)):\n",
    "            model.add(Bidirectional(GRU(\n",
    "                units=hp.Choice(f\"gru_units_{i}\", [64, 128, 256]),\n",
    "                return_sequences=True if i < hp.Int(\"gru_blocks\", 1, 3) - 1 else False)))\n",
    "            model.add(Dropout(hp.Float(f\"dropout_gru_{i}\", 0.1, 0.5, step=0.1)))\n",
    "        \n",
    "        # Dense layers\n",
    "        model.add(Dense(hp.Choice(\"dense_units\", [64, 128, 256]), activation=\"relu\"))\n",
    "        model.add(Dropout(hp.Float(\"dropout_dense\", 0.1, 0.5, step=0.1)))\n",
    "        model.add(Dense(self.num_classes, activation=\"softmax\"))\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=hp.Choice(\"learning_rate\", [1e-3, 1e-4])),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def search_hyperparameters(self, X_train, y_train, X_val, y_val, max_trials=10):\n",
    "        tuner = kt.Hyperband(\n",
    "            self.build_model,\n",
    "            objective=\"val_accuracy\",\n",
    "            max_epochs=50,\n",
    "            directory=\"automl_cnn_gru\",\n",
    "            project_name=\"unsw_nb15\")\n",
    "        \n",
    "        tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]\n",
    "        return best_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Instanciar o pipeline\n",
    "    pipeline = NetworkTrafficPipeline()\n",
    "    \n",
    "    # Baixar o dataset\n",
    "    path = pipeline.download_dataset()\n",
    "    \n",
    "    # Carregar os dados\n",
    "    train_data, test_data = pipeline.load_data()\n",
    "    \n",
    "    # Preprocessamento\n",
    "    preprocessor = Preprocessor(train_data, test_data)\n",
    "    X_train, y_train = preprocessor.train_set_preprocess()\n",
    "    X_test, y_test = preprocessor.test_set_preprocess()\n",
    "    \n",
    "    # Preparar dados sequenciais\n",
    "    def prepare_sequence_data(X, sequence_length):\n",
    "        pad_size = sequence_length - 1\n",
    "        X_padded = np.pad(X, ((pad_size, 0), (0, 0)), mode='edge')\n",
    "        sequences = [X_padded[i:i + sequence_length] for i in range(len(X))]\n",
    "        return np.array(sequences)\n",
    "    \n",
    "    sequence_length = 10\n",
    "    X_train_seq = prepare_sequence_data(X_train, sequence_length)\n",
    "    X_test_seq = prepare_sequence_data(X_test, sequence_length)\n",
    "    \n",
    "    # Divisão treino-validação\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # AutoML com CNN-GRU\n",
    "    cnn_gru_automl = CNNGRUAutoML(input_shape=X_train_seq.shape[2], num_classes=len(np.unique(y_train)))\n",
    "    best_model = cnn_gru_automl.search_hyperparameters(X_train_seq, y_train, X_val_seq, y_val)\n",
    "    \n",
    "    # Avaliação nos dados de teste\n",
    "    test_loss, test_accuracy = best_model.evaluate(X_test_seq, y_test)\n",
    "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
